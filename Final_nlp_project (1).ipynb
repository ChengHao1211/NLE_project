{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5wnTaZrNFC3",
    "outputId": "4c519c59-ae9b-4597-819d-f15754fa2139"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h5eEDdpNcII"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", encoding='ISO-8859-1')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6b7oDyxZNt0g",
    "outputId": "c1591abd-d80a-4375-dd86-212337a46398"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQQqRWFNa-pC",
    "outputId": "67581e15-5ab3-447b-baba-a7bdb8f403ac"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "zpBA4_3YdmtW",
    "outputId": "e152000e-27bb-402e-f127-63823fd98472"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Sltcls0UdqB8",
    "outputId": "31c421cc-a640-4448-bc4a-11c7d8bea2bb"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAGCPW2Mdus2",
    "outputId": "7d835528-d07b-449a-8523-e2748edaee2e"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAqyyr34d5fl",
    "outputId": "6306111d-a9a6-444c-883a-e6fc56948ba1"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of duplicate and non-duplicate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "rHNurXYEd7UG",
    "outputId": "e6d35bfb-d997-4f9a-c9e8-30aa67c57dcc"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(df['is_duplicate'].value_counts())\n",
    "print((df['is_duplicate'].value_counts()/df['is_duplicate'].count())*100)\n",
    "df['is_duplicate'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding total number of unique and repeated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ4TPpaCd8_y",
    "outputId": "d2b99612-a773-429a-b906-c3fd216bb664"
   },
   "outputs": [],
   "source": [
    "\n",
    "qid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
    "print('Number of unique questions',np.unique(qid).shape[0])\n",
    "x = qid.value_counts()>1\n",
    "print('Number of questions getting repeated',x[x].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of Repeated Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "bQc58k5-eOEH",
    "outputId": "bbaf8928-a4dc-4905-9bec-5d5a7dc6f1c9"
   },
   "outputs": [],
   "source": [
    "# Repeated questions histogram\n",
    "plt.hist(qid.value_counts().values, bins=160)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Frequency of Repeated Questions')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbiVZuzIjwah"
   },
   "outputs": [],
   "source": [
    "df['q2_len'] = df['question2'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9P6t0vYkEVk"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finfing the total number of words in question1 and question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h059AK-ekWcW"
   },
   "outputs": [],
   "source": [
    "\n",
    "df['q1_num_words'] = df['question1'].apply(lambda row: len(str(row).split(\" \")))\n",
    "df['q2_num_words'] = df['question2'].apply(lambda row: len(str(row).split(\" \")))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method is used to find the common words in both the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKOkziU4ke4U"
   },
   "outputs": [],
   "source": [
    "def common_words(row):\n",
    "    if pd.isnull(row['question1']) or pd.isnull(row['question2']):\n",
    "        return 0\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n",
    "    return len(w1 & w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMzNJKCjkixK"
   },
   "outputs": [],
   "source": [
    "df['word_common'] = df.apply(common_words, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method is used to find the Total words in both the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeQqL8PYkvbS"
   },
   "outputs": [],
   "source": [
    "def total_words(row):\n",
    "    if pd.isnull(row['question1']) or pd.isnull(row['question2']):\n",
    "        return 0\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n",
    "    return len(w1) + len(w2)\n",
    "\n",
    "df['word_total'] = df.apply(total_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqpNST9dk6zn"
   },
   "outputs": [],
   "source": [
    "df['word_share'] = round(df['word_common']/df['word_total'],2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are the graphs related to above methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "340HVryUlEm-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Drop rows with NaN values in 'q1_len' column\n",
    "df_no_nan = df.dropna(subset=['q1_len'])\n",
    "\n",
    "# Analysis of features\n",
    "sns.distplot(df_no_nan['q1_len'])\n",
    "print('minimum characters', df_no_nan['q1_len'].min())\n",
    "print('maximum characters', df_no_nan['q1_len'].max())\n",
    "print('average num of characters', int(df_no_nan['q1_len'].mean()))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeQfyyHnlQeT"
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.distplot(df[df['is_duplicate'] == 0]['word_common'], label='non duplicate')\n",
    "sns.distplot(df[df['is_duplicate'] == 1]['word_common'], label='duplicate')\n",
    "\n",
    "plt.xlabel('Word Common Count')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfZIpbQsmGl8"
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.distplot(df[df['is_duplicate'] == 0]['word_total'], label='non duplicate')\n",
    "sns.distplot(df[df['is_duplicate'] == 1]['word_total'], label='duplicate')\n",
    "\n",
    "plt.xlabel('Word Total Count')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPKTmrVVmd-z"
   },
   "outputs": [],
   "source": [
    "# Filter out NaN values before plotting\n",
    "non_duplicate_word_share = df[df['is_duplicate'] == 0]['word_share'].dropna()\n",
    "duplicate_word_share = df[df['is_duplicate'] == 1]['word_share'].dropna()\n",
    "\n",
    "# Plot the distribution\n",
    "sns.distplot(non_duplicate_word_share, label='non duplicate')\n",
    "sns.distplot(duplicate_word_share, label='duplicate')\n",
    "\n",
    "plt.xlabel('Word Share')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DShXiP4jeb3y",
    "outputId": "d898d4e6-343a-4a43-c700-7338cf6a92c4"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying nlp concepts to convert text data into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "required_columns = ['question1', 'question2', 'is_duplicate']\n",
    "Quora = df[required_columns]\n",
    "Quora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data by removing punctuation,whitespace,numbers,stopwords ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwmNcI4tedhQ",
    "outputId": "28e469a0-2abc-488a-bb54-2322c3c4344b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # lower text\n",
    "        text = text.lower()\n",
    "        # tokenize text using WhitespaceTokenizer\n",
    "        tokenizer = WhitespaceTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        # remove punctuation\n",
    "        tokens = [word.strip(string.punctuation) for word in tokens]\n",
    "        # remove words that contain numbers\n",
    "        tokens = [word for word in tokens if not any(c.isdigit() for c in word)]\n",
    "        # remove stop words\n",
    "        stop = stopwords.words('english')\n",
    "        stop = [w for w in stop if w not in ['not', 'no']]\n",
    "        tokens = [x for x in tokens if (x not in stop)]\n",
    "        # remove empty tokens\n",
    "        tokens = [t for t in tokens if len(t) > 0]\n",
    "        # remove words with only one letter\n",
    "        tokens = [t for t in tokens if len(t) > 1]\n",
    "        # join all\n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "    else:\n",
    "        # If the input is not a string, return an empty string or handle it as needed\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB2fNFeCeffR"
   },
   "outputs": [],
   "source": [
    "Quora[\"question1_data\"] = Quora[\"question1\"].apply(lambda x: clean_text(x))\n",
    "Quora[\"question2_data\"] = Quora[\"question2\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgbocioQekm2"
   },
   "outputs": [],
   "source": [
    "Quora.drop(Quora.columns[[0, 1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fzm0ZHkie8SC",
    "outputId": "cd23f34e-9e77-4c5e-a49f-6e8e30ec4113"
   },
   "outputs": [],
   "source": [
    "Quora.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_contractions, replace_currency_symbols, remove_hyperlinks, remove_html_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV6oIXtVe_ZX"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def remove_contractions(text):\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "def replace_currency_symbols(text):\n",
    "    currency_symbols = {\n",
    "        \"$\": \"USD\",\n",
    "        \"€\": \"EUR\",\n",
    "        \"£\": \"GBP\",\n",
    "        \"¥\": \"JPY\",\n",
    "        \"₹\": \"INR\",\n",
    "        \"₽\": \"RUB\",  # Russian Ruble\n",
    "        \"₩\": \"KRW\",  # South Korean Won\n",
    "        \"฿\": \"THB\",  # Thai Baht\n",
    "        \"₴\": \"UAH\",  # Ukrainian Hryvnia\n",
    "        \"₦\": \"NGN\"\n",
    "    }\n",
    "    for symbol, currency_name in currency_symbols.items():\n",
    "        text = text.replace(symbol, currency_name)\n",
    "    return text\n",
    "\n",
    "def remove_hyperlinks(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "def process_column(column):\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        processed_column = pool.map(remove_contractions, column)\n",
    "        processed_column = pool.map(replace_currency_symbols, processed_column)\n",
    "        processed_column = pool.map(remove_hyperlinks, processed_column)\n",
    "        processed_column = pool.map(remove_html_tags, processed_column)\n",
    "    return processed_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW1Zob2GfCBB"
   },
   "outputs": [],
   "source": [
    "Quora['question1_data'] = process_column(Quora['question1_data'])\n",
    "Quora['question2_data'] = process_column(Quora['question2_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIFFfySVfPRi",
    "outputId": "9085e029-0f3e-4e8b-c936-6c1b29a3832d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Combine all questions for training Word2Vec\n",
    "all_questions = list(Quora['question1_data']) + list(Quora['question2_data'])\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=all_questions, size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9awK6KIftHG"
   },
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "def word2vec_vectorizer(data, model, num_features):\n",
    "    if hasattr(model.wv, 'index_to_key'):\n",
    "        vocabulary = set(model.wv.index_to_key)\n",
    "    else:\n",
    "        vocabulary = set(model.wv.index2word)\n",
    "\n",
    "    features = [average_word_vectors(question, model, vocabulary, num_features) for question in data]\n",
    "    return np.array(features)\n",
    "\n",
    "# Vectorize questions\n",
    "X = word2vec_vectorizer(Quora['question1_data'], word2vec_model, 100)\n",
    "Y = word2vec_vectorizer(Quora['question2_data'], word2vec_model, 100)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(np.hstack((X, Y)), Quora['is_duplicate'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU_NuOBxfvFr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define the RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use accuracy as the scoring metric for grid search\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring=scorer, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = best_clf.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming clf is your RandomForestClassifier instance\n",
    "cv_scores = cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Score:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'ROC AUC: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train, X_test, Y_train, Y_test are properly defined\n",
    "clf_svc = SVC(random_state=42)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_svc = clf_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svc = accuracy_score(Y_test, predictions_svc)\n",
    "print(f'Accuracy: {accuracy_svc}')\n",
    "print(classification_report(Y_test, predictions_svc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train, X_test, Y_train, Y_test are properly defined\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_knn = clf_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_knn = accuracy_score(Y_test, predictions_knn)\n",
    "print(f'Accuracy: {accuracy_knn}')\n",
    "print(classification_report(Y_test, predictions_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train, X_test, Y_train, Y_test are properly defined\n",
    "clfxgb = XGBClassifier(random_state=42)\n",
    "clfxgb.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_xgb = clfxgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(Y_test, predictions_xgb)\n",
    "print(f'Accuracy: {accuracy_xgb}')\n",
    "print(classification_report(Y_test, predictions_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
